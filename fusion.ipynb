{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "p4U8rQW59irD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import TransformerConv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch import tensor\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch.utils.data as data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "yHV2yinbaXBN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def load_data(dataset, device):\n",
        "    data_file = f\"./original_datasets/{dataset}/{dataset}_train\"\n",
        "    file = open(data_file, \"r\")\n",
        "    node_types = set()\n",
        "    label_types = set()\n",
        "    tr_len = 0\n",
        "    for line in file:\n",
        "        tr_len += 1\n",
        "        smiles = line.split(\"\\t\")[1]\n",
        "        s = []\n",
        "        mol = AllChem.MolFromSmiles(smiles)\n",
        "        for atom in mol.GetAtoms():\n",
        "            s.append(atom.GetAtomicNum())\n",
        "        node_types |= set(s)\n",
        "        label = line.split(\"\\t\")[2][:-1]\n",
        "        label_types.add(label)\n",
        "    file.close()\n",
        "\n",
        "    te_len = 0\n",
        "    data_file = f\"./original_datasets/{dataset}/{dataset}_test\"\n",
        "    file = open(data_file, \"r\")\n",
        "    for line in file:\n",
        "        te_len += 1\n",
        "        smiles = line.split(\"\\t\")[1]\n",
        "        s = []\n",
        "        mol = AllChem.MolFromSmiles(smiles)\n",
        "        for atom in mol.GetAtoms():\n",
        "            s.append(atom.GetAtomicNum())\n",
        "        node_types |= set(s)\n",
        "        label = line.split(\"\\t\")[2][:-1]\n",
        "        label_types.add(label)\n",
        "    file.close()\n",
        "\n",
        "    #print(tr_len)\n",
        "    #print(te_len)\n",
        "\n",
        "    node2index = {n: i for i, n in enumerate(node_types)}\n",
        "    label2index = {l: i for i, l in enumerate(label_types)}\n",
        "\n",
        "    #print(node2index)\n",
        "    #print(label2index)\n",
        "\n",
        "    data_file = f\"./original_datasets/{dataset}/{dataset}_train\"\n",
        "    file = open(data_file, \"r\")\n",
        "    train_adjlists = []\n",
        "    train_features = []\n",
        "    train_sequence = []\n",
        "    train_labels = torch.zeros(tr_len)\n",
        "    for line in file:\n",
        "        smiles = line.split(\"\\t\")[1]\n",
        "        label = line.split(\"\\t\")[2][:-1]\n",
        "        mol = AllChem.MolFromSmiles(smiles)\n",
        "        feature = torch.zeros(len(mol.GetAtoms()), len(node_types))\n",
        "\n",
        "        l = 0\n",
        "        smiles_seq = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            feature[l, node2index[atom.GetAtomicNum()]] = 1\n",
        "            smiles_seq.append(node2index[atom.GetAtomicNum()])\n",
        "            l += 1\n",
        "        adj_list = defaultdict(list)\n",
        "        for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            typ = bond.GetBondType()\n",
        "            adj_list[i].append(j)\n",
        "            adj_list[j].append(i)\n",
        "            if typ == Chem.rdchem.BondType.DOUBLE:\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "            elif typ == Chem.rdchem.BondType.TRIPLE:\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "\n",
        "        train_labels[len(train_adjlists)]= int(label2index[label])\n",
        "        #print(train_labels)\n",
        "        train_adjlists.append(adj_list)\n",
        "        train_features.append(torch.FloatTensor(feature).to(device))\n",
        "        train_sequence.append(torch.tensor(smiles_seq))\n",
        "    file.close()\n",
        "\n",
        "    data_file = f\"./original_datasets/{dataset}/{dataset}_test\"\n",
        "    file = open(data_file, \"r\")\n",
        "    test_adjlists = []\n",
        "    test_features = []\n",
        "    test_sequence = []\n",
        "    test_labels = np.zeros(te_len)\n",
        "    for line in file:\n",
        "        smiles = line.split(\"\\t\")[1]\n",
        "        # print(smiles)\n",
        "        label = line.split(\"\\t\")[2][:-1]\n",
        "        mol = AllChem.MolFromSmiles(smiles)\n",
        "        feature = torch.zeros(len(mol.GetAtoms()), len(node_types))\n",
        "        l = 0\n",
        "        smiles_seq = []\n",
        "        for atom in mol.GetAtoms():\n",
        "            feature[l, node2index[atom.GetAtomicNum()]] = 1\n",
        "            smiles_seq.append(node2index[atom.GetAtomicNum()])\n",
        "            l += 1\n",
        "        adj_list = defaultdict(list)\n",
        "        for bond in mol.GetBonds():\n",
        "            i = bond.GetBeginAtomIdx()\n",
        "            j = bond.GetEndAtomIdx()\n",
        "            typ = bond.GetBondType()\n",
        "            adj_list[i].append(j)\n",
        "            adj_list[j].append(i)\n",
        "            if typ == Chem.rdchem.BondType.DOUBLE:\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "            elif typ == Chem.rdchem.BondType.TRIPLE:\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "                adj_list[i].append(j)\n",
        "                adj_list[j].append(i)\n",
        "\n",
        "        test_labels[len(test_adjlists)] = int(label2index[label])\n",
        "        test_adjlists.append(adj_list)\n",
        "        test_features.append(torch.FloatTensor(feature).to(device))\n",
        "        test_sequence.append(torch.tensor(smiles_seq))\n",
        "    file.close()\n",
        "    train_data = {}\n",
        "    train_data['adj_lists'] = train_adjlists\n",
        "    train_data['features'] = train_features\n",
        "    # Pad train_sequence to length 100\n",
        "    padded_train_sequence = []\n",
        "    for seq in train_sequence:\n",
        "      padded_seq = torch.nn.functional.pad(seq, (0, 100 - len(seq)), 'constant', 0)\n",
        "      padded_train_sequence.append(padded_seq)\n",
        "      train_data['sequence'] = padded_train_sequence\n",
        "    test_data = {}\n",
        "    test_data['adj_lists'] = test_adjlists\n",
        "    test_data['features'] = test_features\n",
        "    padded_test_sequence = []\n",
        "    for seq in test_sequence:\n",
        "      padded_seq = torch.nn.functional.pad(seq, (0, 100 - len(seq)), 'constant', 0)\n",
        "      padded_test_sequence.append(padded_seq)\n",
        "      test_data['sequence'] = padded_test_sequence\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "2bJwYJKB9wzS"
      },
      "outputs": [],
      "source": [
        "train_data, train_labels, test_data, test_labels=load_data(\"logp\",device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m07tABtcbVBt",
        "outputId": "aa6aaa61-5102-4b38-b7d1-f75ebc761808"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 9, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data['sequence'][2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NqRk_GmJ-Kll",
        "outputId": "6eadb4bd-06d8-4c66-a4ab-1a7a55af3696"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8960\\3184463301.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  logp_data_list_train = [Data(x=torch.tensor(features, dtype=torch.float),\n",
            "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8960\\3184463301.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y=torch.tensor(label, dtype=torch.float))\n",
            "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8960\\3184463301.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  logp_data_list_test= [Data(x=torch.tensor(features, dtype=torch.float),\n",
            "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8960\\3184463301.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y=torch.tensor(label, dtype=torch.float))\n"
          ]
        }
      ],
      "source": [
        "logp_input_dim_train = train_data['features'][0].size(-1)\n",
        "logp_input_dim_test = test_data['features'][0].size(-1)\n",
        "def adj_list_to_adj_matrix(adj_list):\n",
        "    num_nodes = len(adj_list)\n",
        "    adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
        "    for node, neighbors in adj_list.items():\n",
        "        for neighbor in neighbors:\n",
        "            adj_matrix[node][neighbor] = 1.0\n",
        "            adj_matrix[neighbor][node] = 1.0\n",
        "    return adj_matrix\n",
        "logp_adj_matrices_train = [adj_list_to_adj_matrix(adj_list) for adj_list in train_data['adj_lists']]\n",
        "logp_adj_matrices_test = [adj_list_to_adj_matrix(adj_list) for adj_list in test_data['adj_lists']]\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Combine data and sequence into one tensor for both train and test data\n",
        "logp_data_sequence_train = torch.stack(train_data['sequence'])\n",
        "logp_data_sequence_test = torch.stack(test_data['sequence'])\n",
        "\n",
        "logp_data_list_train = [Data(x=torch.tensor(features, dtype=torch.float),\n",
        "                  edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
        "                  y=torch.tensor(label, dtype=torch.float))\n",
        "             for features, adj_matrix, label in zip(train_data['features'], logp_adj_matrices_train, train_labels)]\n",
        "logp_data_list_test= [Data(x=torch.tensor(features, dtype=torch.float),\n",
        "                  edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
        "                  y=torch.tensor(label, dtype=torch.float))\n",
        "             for features, adj_matrix, label in zip(test_data['features'], logp_adj_matrices_test, train_labels)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ui66hoXM2JSC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_list, sequence_list):\n",
        "        self.data_list = data_list\n",
        "        self.sequence_list = sequence_list\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        data = self.data_list[index]\n",
        "        sequence = self.sequence_list[index]\n",
        "        return data, sequence\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_list)\n",
        "\n",
        "\n",
        "def custom_collate(batch):\n",
        "  if isinstance(batch[0], Data):\n",
        "    # Convert each Data object in the batch to a dictionary.\n",
        "    batch = [data.__dict__ for data in batch]\n",
        "    # Return a new Data object with the dictionaries as its attributes.\n",
        "    return Data(**batch[0])\n",
        "  else:\n",
        "    # Handle other types of data as needed.\n",
        "    pass\n",
        "# Create datasets\n",
        "train_dataset = CustomDataset(logp_data_list_train, train_data['sequence'])\n",
        "test_dataset = CustomDataset(logp_data_list_test, test_data['sequence'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False,collate_fn=custom_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False,collate_fn=custom_collate)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU5zyzon1W3E",
        "outputId": "09f18fb7-0f66-4b74-a392-0ff1e6bb0af3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Data(x=[34, 12], edge_index=[2, 72], y=1.0),\n",
              " tensor([1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3, 2, 1, 1,\n",
              "         1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0]))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p57tHQoS2g3F",
        "outputId": "02cd4ba2-3c75-4755-ec8b-d288718f691d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 1, 1, 1, 1, 1, 1, 3, 2, 2, 1, 1, 1, 1, 1, 1, 9, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "third_tensor = train_dataset[2][1]\n",
        "third_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxmzC4il3NpU",
        "outputId": "ee12fabe-b894-447c-d8bc-9228f3fb3b64"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data=train_dataset[0][0]\n",
        "data.y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "8f1gZDkj_6dG"
      },
      "outputs": [],
      "source": [
        "class GCNConvLayer(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, heads, edge_weight=None):\n",
        "        super(GCNConvLayer, self).__init__()\n",
        "        self.conv = TransformerConv(in_channels, out_channels, heads)\n",
        "        self.edge_weight = edge_weight\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        return self.conv(x, edge_index)\n",
        "\n",
        "\n",
        "class GCN_2l(nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, heads, dropout_rate=0.5):\n",
        "        super(GCN_2l, self).__init__()\n",
        "        self.conv1 = GCNConvLayer(in_channels, hidden_channels, heads)\n",
        "        self.conv2 = GCNConvLayer(hidden_channels*heads, out_channels, heads)\n",
        "        self.linear=nn.Linear(heads,1)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        #print(\"before conv1: \", x.shape)\n",
        "        x = self.conv1(x, edge_index)\n",
        "        #print(\"after conv1: \", x.shape)\n",
        "        x = torch.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        #print(\"before conv2: \", x.shape)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        #print(\"after conv2: \", x.shape)\n",
        "        x=self.linear(x)\n",
        "        #print(x)\n",
        "        #print(x.mean(dim=0, keepdim=True))\n",
        "        return x.mean(dim=0, keepdim=True)  # Aggregating to a single output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Rk-QAmttAfwX"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward, max_length=100):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward), num_layers=num_encoder_layers)\n",
        "        self.fc = nn.Linear(d_model, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = torch.mean(x, dim=0)\n",
        "        x = self.fc(x)\n",
        "        return  x.mean(dim=0, keepdim=True)\n",
        "\n",
        "\n",
        "# Define hyperparameters\n",
        "vocab_size = 100\n",
        "d_model = 100\n",
        "nhead = 4\n",
        "num_encoder_layers = 3\n",
        "dim_feedforward = 512\n",
        "max_length = 100\n",
        "batch_size = 1\n",
        "num_epochs = 100\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "877xuBwFGy-r"
      },
      "outputs": [],
      "source": [
        "class FusionModel(nn.Module):\n",
        "    def __init__(self, graph_model, sequence_model):\n",
        "        super(FusionModel, self).__init__()\n",
        "        self.graph_model = graph_model\n",
        "        self.sequence_model = sequence_model\n",
        "        # Attention layer\n",
        "        self.attention = nn.Linear(graph_model.linear.out_features + sequence_model.fc.out_features, 1)\n",
        "        # Fusion layer\n",
        "        self.fusion_linear = nn.Linear(graph_model.linear.out_features + sequence_model.fc.out_features, 1)\n",
        "\n",
        "    def forward(self, graph_data, sequence_data):\n",
        "        graph_embedding = self.graph_model(graph_data)\n",
        "        sequence_embedding = self.sequence_model(sequence_data)\n",
        "\n",
        "        # Reshape sequence embedding to have two dimensions\n",
        "        sequence_embedding = sequence_embedding.unsqueeze(0)\n",
        "\n",
        "\n",
        "        # Concatenate embeddings\n",
        "        combined_embedding = torch.cat((graph_embedding, sequence_embedding), dim=1)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = torch.sigmoid(self.attention(combined_embedding))\n",
        "\n",
        "        # Apply attention to sequence embedding\n",
        "        weighted_sequence_embedding = attention_weights * sequence_embedding\n",
        "\n",
        "        # Fusion\n",
        "        fused_embedding = torch.cat((graph_embedding, weighted_sequence_embedding), dim=1)\n",
        "\n",
        "        # Apply fusion layer\n",
        "        output = self.fusion_linear(fused_embedding)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8GDbl8Vu8PZB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "graph_model = GCN_2l(in_channels=logp_input_dim_train, hidden_channels=64, out_channels=1, heads=4)\n",
        "sequence_model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward)\n",
        "fusion_model = FusionModel(graph_model, sequence_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MmOC8mVFluj",
        "outputId": "3ae14c45-1c2d-45e0-be66-88f395b645c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100, Epoch Accuracy: 0.7855\n",
            "Epoch 2/100, Epoch Accuracy: 0.8538\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[31], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# Update weights\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Compute epoch accuracy\u001b[39;00m\n\u001b[0;32m     43\u001b[0m epoch_accuracy \u001b[38;5;241m=\u001b[39m total_correct \u001b[38;5;241m/\u001b[39m total_samples\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\optim\\adam.py:441\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    439\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m--> 441\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_avg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenom\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mstep_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(params[i]):\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "# Define the loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = optim.Adam(fusion_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    for data_batch in train_dataset:\n",
        "        graph_data_batch = data_batch[0]\n",
        "        sequence_inputs  = data_batch[1]\n",
        "        sequence_targets=graph_data_batch.y\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = fusion_model(graph_data_batch, sequence_inputs)\n",
        "\n",
        "        # Compute binary predictions\n",
        "        binary_predictions = (output > 0.5).float()\n",
        "\n",
        "        # Compute batch accuracy\n",
        "        batch_correct = (binary_predictions == sequence_targets).sum().item()\n",
        "        total_correct += batch_correct\n",
        "        total_samples += 1\n",
        "\n",
        "        sequence_targets = sequence_targets.view(-1, 1)\n",
        "        # Compute loss\n",
        "        loss = criterion(output, sequence_targets)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    # Compute epoch accuracy\n",
        "    epoch_accuracy = total_correct / total_samples\n",
        "    print(f\"Epoch {epoch+1}/{100}, Epoch Accuracy: {epoch_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

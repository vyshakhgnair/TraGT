{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import Chem\n",
    "\n",
    "from torch_geometric.data import DataLoader,Data\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adj_list_to_adj_matrix(adj_list):\n",
    "    num_nodes = len(adj_list)\n",
    "    adj_matrix = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
    "    for node, neighbors in adj_list.items():\n",
    "        for neighbor in neighbors:\n",
    "            adj_matrix[node][neighbor] = 1.0\n",
    "            adj_matrix[neighbor][node] = 1.0\n",
    "    return adj_matrix\n",
    "\n",
    "def pad_sequence_to_length(sequence, length):\n",
    "    if len(sequence) < length:\n",
    "        pad_size = length - len(sequence)\n",
    "        padding = torch.zeros(pad_size, *sequence.size()[1:], dtype=sequence.dtype, device=sequence.device)\n",
    "        return torch.cat((sequence, padding), dim=0)\n",
    "    else:\n",
    "        return sequence[:length]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class SMILESDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    \n",
    "def load_data(dataset, device):\n",
    "\n",
    "    data_file = f\"./original_datasets/{dataset}/{dataset}_train\"\n",
    "    file = open(data_file, \"r\")\n",
    "    node_types = set()\n",
    "    label_types = set()\n",
    "    tr_len = 0\n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        #break\n",
    "        tr_len += 1\n",
    "        smiles = line.split(\"\\t\")[1]\n",
    "        s = []\n",
    "        mol = AllChem.MolFromSmiles(smiles)\n",
    "        for atom in mol.GetAtoms():\n",
    "            s.append(atom.GetAtomicNum())\n",
    "        node_types |= set(s)\n",
    "        label = line.split(\"\\t\")[2][:-1]\n",
    "        #print(label)\n",
    "        label_types.add(label)\n",
    "        #print(label_types)\n",
    "    file.close()\n",
    "\n",
    "    te_len = 0\n",
    "    data_file = f\"./original_datasets/{dataset}/{dataset}_test\"\n",
    "    file = open(data_file, \"r\")\n",
    "    for line in file:\n",
    "        te_len += 1\n",
    "        smiles = line.split(\"\\t\")[1]\n",
    "        s = []\n",
    "        mol = AllChem.MolFromSmiles(smiles)\n",
    "        for atom in mol.GetAtoms():\n",
    "            s.append(atom.GetAtomicNum())\n",
    "        node_types |= set(s)\n",
    "        label = line.split(\"\\t\")[2][:-1]\n",
    "        label_types.add(label)\n",
    "    file.close()\n",
    "\n",
    "    print(tr_len)\n",
    "    print(te_len)\n",
    "\n",
    "    node2index = {n: i for i, n in enumerate(node_types)}\n",
    "    label2index = {i: i for i in label_types}\n",
    "\n",
    "    print(node2index)\n",
    "    print(label2index)\n",
    "\n",
    "    data_file = f\"./original_datasets/{dataset}/{dataset}_train\"\n",
    "    file = open(data_file, \"r\")\n",
    "    train_adjlists = []\n",
    "    train_features = []\n",
    "    train_sequence = []\n",
    "    train_labels = torch.zeros(tr_len)\n",
    "    for line in file:\n",
    "        smiles = line.split(\"\\t\")[1]\n",
    "        label = line.split(\"\\t\")[2][:-1]\n",
    "        mol = AllChem.MolFromSmiles(smiles)\n",
    "        feature = torch.zeros(len(mol.GetAtoms()), len(node_types))\n",
    "\n",
    "        l = 0\n",
    "        smiles_seq = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feature[l, node2index[atom.GetAtomicNum()]] = 1\n",
    "            smiles_seq.append(node2index[atom.GetAtomicNum()])\n",
    "            l += 1\n",
    "        adj_list = defaultdict(list)\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            typ = bond.GetBondType()\n",
    "            adj_list[i].append(j)\n",
    "            adj_list[j].append(i)\n",
    "            if typ == Chem.rdchem.BondType.DOUBLE:\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "            elif typ == Chem.rdchem.BondType.TRIPLE:\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "\n",
    "        train_labels[len(train_adjlists)]= int(label2index[label])\n",
    "        #print(\"train:\",train_labels)\n",
    "        train_adjlists.append(adj_list)\n",
    "        train_features.append(torch.FloatTensor(feature).to(device))\n",
    "        train_sequence.append(torch.tensor(smiles_seq))\n",
    "    file.close()\n",
    "\n",
    "    data_file = f\"./original_datasets/{dataset}/{dataset}_test\"\n",
    "    file = open(data_file, \"r\")\n",
    "    test_adjlists = []\n",
    "    test_features = []\n",
    "    test_sequence = []\n",
    "    test_labels = np.zeros(te_len)\n",
    "    for line in file:\n",
    "        smiles = line.split(\"\\t\")[1]\n",
    "        label = line.split(\"\\t\")[2][:-1]\n",
    "        mol = AllChem.MolFromSmiles(smiles)\n",
    "        feature = torch.zeros(len(mol.GetAtoms()), len(node_types))\n",
    "        l = 0\n",
    "        smiles_seq = []\n",
    "        for atom in mol.GetAtoms():\n",
    "            feature[l, node2index[atom.GetAtomicNum()]] = 1\n",
    "            smiles_seq.append(node2index[atom.GetAtomicNum()])\n",
    "            l += 1\n",
    "        adj_list = defaultdict(list)\n",
    "        for bond in mol.GetBonds():\n",
    "            i = bond.GetBeginAtomIdx()\n",
    "            j = bond.GetEndAtomIdx()\n",
    "            typ = bond.GetBondType()\n",
    "            adj_list[i].append(j)\n",
    "            adj_list[j].append(i)\n",
    "            if typ == Chem.rdchem.BondType.DOUBLE:\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "            elif typ == Chem.rdchem.BondType.TRIPLE:\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "                adj_list[i].append(j)\n",
    "                adj_list[j].append(i)\n",
    "\n",
    "        test_labels[len(test_adjlists)] = int(label2index[label])\n",
    "        test_adjlists.append(adj_list)\n",
    "        test_features.append(torch.FloatTensor(feature).to(device))\n",
    "        test_sequence.append(torch.tensor(smiles_seq))\n",
    "    file.close()\n",
    "\n",
    "    train_data = {}\n",
    "    train_data['adj_lists'] = train_adjlists\n",
    "    train_data['features'] = train_features\n",
    "    train_data['sequence'] = train_sequence\n",
    "\n",
    "    test_data = {}\n",
    "    test_data['adj_lists'] = test_adjlists\n",
    "    test_data['features'] = test_features\n",
    "    test_data['sequence'] = test_sequence\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, graph_data, sequence_data, labels):\n",
    "        self.graph_data = graph_data\n",
    "        self.sequence_data = sequence_data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graph_data[idx], self.sequence_data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000\n",
      "1000\n",
      "{5: 0, 6: 1, 7: 2, 8: 3, 9: 4, 14: 5, 15: 6, 16: 7, 17: 8, 35: 9, 50: 10, 53: 11}\n",
      "{'0': '0', '1': '1'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_23296\\2362275680.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(features, dtype=torch.float),\n",
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_23296\\2362275680.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(label, dtype=torch.float)\n",
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_23296\\2362275680.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x=torch.tensor(features, dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph data: torch.Size([21, 12]) torch.Size([2, 44]) tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "Sequence data: torch.Size([100]) tensor([1, 3, 1, 1, 1, 1, 2, 2, 1, 3, 1, 1, 1, 1, 1, 7, 1, 1, 9, 1, 3, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], device='cuda:0')\n",
      "Labels: tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "d_name = \"logp\"\n",
    "if torch.cuda.is_available():  # Check GPU availability\n",
    "    device = torch.device(\"cuda:0\")  # Set device to GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "train_data, train_labels, test_data, test_labels = load_data(\"logp\", device=device)  # Pass device argument\n",
    "\n",
    "# Move data tensors to GPU\n",
    "train_data['sequence'] = [torch.Tensor(seq).to(device) for seq in train_data['sequence']]\n",
    "test_data['sequence'] = [torch.Tensor(seq).to(device) for seq in test_data['sequence']]\n",
    "\n",
    "padded_train_sequence = [pad_sequence_to_length(tensor, length=100) for tensor in train_data['sequence']]\n",
    "padded_test_sequence = [pad_sequence_to_length(tensor, length=100) for tensor in test_data['sequence']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim_train = train_data['features'][0].size(-1)\n",
    "input_dim_test = test_data['features'][0].size(-1)\n",
    "\n",
    "adj_matrices_train = [adj_list_to_adj_matrix(adj_list) for adj_list in train_data['adj_lists']]\n",
    "adj_matrices_test = [adj_list_to_adj_matrix(adj_list) for adj_list in test_data['adj_lists']]\n",
    "\n",
    "data_list_train = [\n",
    "    Data(\n",
    "        x=torch.tensor(features, dtype=torch.float),\n",
    "        edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
    "        y=torch.tensor(label, dtype=torch.float)\n",
    "    )\n",
    "    for features, adj_matrix, label in zip(train_data['features'], adj_matrices_train, train_labels)\n",
    "]\n",
    "data_list_test = [\n",
    "    Data(\n",
    "        x=torch.tensor(features, dtype=torch.float),\n",
    "        edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
    "        y=torch.tensor(label, dtype=torch.float)\n",
    "    )\n",
    "    for features, adj_matrix, label in zip(test_data['features'], adj_matrices_test, test_labels)\n",
    "]\n",
    "\n",
    "\n",
    "# Combine both graph and sequence data into a single dataset\n",
    "train_data = CustomDataset(data_list_train, padded_train_sequence, train_labels)\n",
    "test_data = CustomDataset(data_list_test, padded_test_sequence, test_labels)\n",
    "\n",
    "# Create a single loader for the combined dataset\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "# Loop through the combined loader\n",
    "for graph_data, seq_data, labels in train_dataloader:\n",
    "    print('Graph data:', graph_data.x.shape, graph_data.edge_index.shape,graph_data.x)\n",
    "    print('Sequence data:', seq_data[0].shape, seq_data[0])\n",
    "    print('Labels:', labels)\n",
    "    break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!\n",
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

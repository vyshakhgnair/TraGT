{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TraGT.load_data import load_data, load_data_long,CustomDataset,adj_list_to_adj_matrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from TraGT.seq_models import TransformerModel\n",
    "from TraGT.graph_models import Graph_Transformer\n",
    "from TraGT_v2_recon.fusion_model import FusionModel\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_name='bbbp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:32] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[15:44:34] WARNING: not removing hydrogen atom without neighbors\n",
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8436\\1327122607.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_list_train = [Data(x=torch.tensor(features, dtype=torch.float),\n",
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8436\\1327122607.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y=torch.tensor(label, dtype=torch.float))\n",
      "C:\\Users\\vysha\\AppData\\Local\\Temp\\ipykernel_8436\\1327122607.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_list_test = [Data(x=torch.tensor(features, dtype=torch.float),\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, test_data, test_labels = load_data_long(data_name, device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"))\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "input_dim_train = train_data['features'][0].size(-1)\n",
    "input_dim_test = test_data['features'][0].size(-1)\n",
    "\n",
    "\n",
    "adj_matrices_train = [adj_list_to_adj_matrix(adj_list) for adj_list in train_data['adj_lists']]\n",
    "adj_matrices_test = [adj_list_to_adj_matrix(adj_list) for adj_list in test_data['adj_lists']]\n",
    "\n",
    "\n",
    "data_list_train = [Data(x=torch.tensor(features, dtype=torch.float),\n",
    "                        edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
    "                        y=torch.tensor(label, dtype=torch.float))\n",
    "                    for features, adj_matrix, label in zip(train_data['features'], adj_matrices_train, train_labels)]\n",
    "data_list_test = [Data(x=torch.tensor(features, dtype=torch.float),\n",
    "                                edge_index=torch.nonzero(adj_matrix, as_tuple=False).t().contiguous(),\n",
    "                                y=torch.tensor(label, dtype=torch.float))\n",
    "                            for features, adj_matrix, label in zip(test_data['features'], adj_matrices_test, test_labels)]\n",
    "\n",
    "train_dataset = CustomDataset(data_list_train, train_data['sequence'])\n",
    "test_dataset = CustomDataset(data_list_test, test_data['sequence'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FusionModel:\n\tUnexpected key(s) in state_dict: \"sequence_model.decoder.lstm.weight_ih_l0\", \"sequence_model.decoder.lstm.weight_hh_l0\", \"sequence_model.decoder.lstm.bias_ih_l0\", \"sequence_model.decoder.lstm.bias_hh_l0\", \"sequence_model.decoder.fc.weight\", \"sequence_model.decoder.fc.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m fusion_model \u001b[38;5;241m=\u001b[39m FusionModel(graph_model, sequence_model)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Load the state dict into the model\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mfusion_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FusionModel:\n\tUnexpected key(s) in state_dict: \"sequence_model.decoder.lstm.weight_ih_l0\", \"sequence_model.decoder.lstm.weight_hh_l0\", \"sequence_model.decoder.lstm.bias_ih_l0\", \"sequence_model.decoder.lstm.bias_hh_l0\", \"sequence_model.decoder.fc.weight\", \"sequence_model.decoder.fc.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "vocab_size = 100\n",
    "d_model = 100\n",
    "nhead = 4\n",
    "num_encoder_layers = 3\n",
    "dim_feedforward = 512\n",
    "max_length = 100\n",
    "batch_size = 1\n",
    "num_epochs = 100\n",
    "model_path = 'saved_models/bbbp_2024-04-28_23-06-49/test_best_model_91.710.pth'\n",
    "\n",
    "graph_model = Graph_Transformer(in_channels=input_dim_train, hidden_channels=64, out_channels=1, heads=4).to(device)\n",
    "\n",
    "sequence_model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, dim_feedforward).to(device)\n",
    "\n",
    "fusion_model = FusionModel(graph_model, sequence_model).to(device)\n",
    "\n",
    "# Load the state dict into the model\n",
    "fusion_model.load_state_dict(torch.load(model_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n",
      "\u001b[0;32m     10\u001b[0m sequence_inputs \u001b[38;5;241m=\u001b[39m data_batch[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;32m     11\u001b[0m sequence_targets \u001b[38;5;241m=\u001b[39m graph_data_batch\u001b[38;5;241m.\u001b[39my\n",
      "\u001b[1;32m---> 13\u001b[0m output, reconstructed_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mfusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     14\u001b[0m binary_predictions \u001b[38;5;241m=\u001b[39m (output \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;32m     16\u001b[0m batch_correct \u001b[38;5;241m=\u001b[39m (binary_predictions \u001b[38;5;241m==\u001b[39m sequence_targets)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store outputs and labels\n",
    "outputs_test = []\n",
    "true_labels_test = []\n",
    "pred_probs_test = []\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for data_batch in test_dataset:\n",
    "    graph_data_batch = data_batch[0]\n",
    "    sequence_inputs = data_batch[1]\n",
    "    sequence_targets = graph_data_batch.y\n",
    "                \n",
    "    output, reconstructed_sequence = fusion_model(graph_data_batch, sequence_inputs)\n",
    "    binary_predictions = (output >= 0.5).float()\n",
    "                \n",
    "    batch_correct = (binary_predictions == sequence_targets).sum().item()\n",
    "    total_correct += batch_correct\n",
    "    total_samples += 1\n",
    "                \n",
    "    true_labels_test.append(sequence_targets.cpu().numpy().reshape(-1))\n",
    "    pred_probs_test.append(output.detach().cpu().numpy()[0])\n",
    "    outputs_test.append(output.detach().cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "outputs_test = np.concatenate(outputs_test, axis=0)\n",
    "true_labels_test = np.concatenate(true_labels_test, axis=0)\n",
    "\n",
    "# Use t-SNE to reduce dimensionality to 2D\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "outputs_test_2d = tsne.fit_transform(outputs_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 5))\n",
    "scatter = plt.scatter(outputs_test_2d[:, 0], outputs_test_2d[:, 1], c=true_labels_test)\n",
    "plt.title('t-SNE plot of test data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m sequence_inputs \u001b[38;5;241m=\u001b[39m data_batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     11\u001b[0m sequence_targets \u001b[38;5;241m=\u001b[39m graph_data_batch\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m---> 13\u001b[0m output, reconstructed_sequence \u001b[38;5;241m=\u001b[39m \u001b[43mfusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_data_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m binary_predictions \u001b[38;5;241m=\u001b[39m (output \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     16\u001b[0m batch_correct \u001b[38;5;241m=\u001b[39m (binary_predictions \u001b[38;5;241m==\u001b[39m sequence_targets)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "\u001b[1;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store outputs and labels\n",
    "outputs_test = []\n",
    "true_labels_test = []\n",
    "pred_probs_test = []\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "for data_batch in test_dataset:\n",
    "    graph_data_batch = data_batch[0]\n",
    "    sequence_inputs = data_batch[1]\n",
    "    sequence_targets = graph_data_batch.y\n",
    "                \n",
    "    output, reconstructed_sequence = fusion_model(graph_data_batch, sequence_inputs)\n",
    "    binary_predictions = (output >= 0.5).float()\n",
    "                \n",
    "    batch_correct = (binary_predictions == sequence_targets).sum().item()\n",
    "    total_correct += batch_correct\n",
    "    total_samples += 1\n",
    "                \n",
    "    true_labels_test.append(sequence_targets.cpu().numpy().reshape(-1))\n",
    "    pred_probs_test.append(output.detach().cpu().numpy()[0])\n",
    "    outputs_test.append(output.detach().cpu().numpy())\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "outputs_test = np.concatenate(outputs_test, axis=0)\n",
    "true_labels_test = np.concatenate(true_labels_test, axis=0)\n",
    "\n",
    "# Use t-SNE to reduce dimensionality to 2D\n",
    "tsne = TSNE(n_components=2, random_state=0)\n",
    "outputs_test_2d = tsne.fit_transform(outputs_test)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(6, 5))\n",
    "scatter = plt.scatter(outputs_test_2d[:, 0], outputs_test_2d[:, 1], c=true_labels_test)\n",
    "plt.title('t-SNE plot of test data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend(*scatter.legend_elements())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
